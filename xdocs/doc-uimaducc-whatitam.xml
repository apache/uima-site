<?xml version="1.0" encoding="ISO-8859-1"?>

<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at
 
 http://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
-->

<document>

 <properties>
  <title>Getting Started: Apache UIMA DUCC</title>
  <author email="dev@uima.apache.org">
   Apache UIMA Documentation Team
  </author>
 </properties>

 <body>
  <section name="Getting Started: Apache UIMA DUCC">
   <p>
This guide is intended to be a user oriented overview of Apache UIMA&#0153; DUCC.
<br>DUCC is short for Distributed UIMA Cluster Computing.</br>
   </p>

   <subsection name="High Level Overview of the UIMA DUCC Platform">
   <p>
DUCC is a Linux cluster controller designed to scale out any UIMA pipeline
for high throughput collection processing jobs as well as for low latency
real-tme applications.
Building on UIMA-AS, DUCC is particularly well suited to run large memory Java 
analytics in multiple threads in order to fully utilize multicore machines.
DUCC manages the life cycle of all processes deployed across the cluster.
Non-UIMA processes such as tomcat servers or VNC sessions can also be managed.
   </p>
   <p>
DUCC has an extensive web interface providing details on all user activity
across the cluster. Because DUCC is built for UIMA-based analytics from the
ground up it automatically makes available such details as what annotators are
currently initializing as well as the timing breakdown for each primitive annotator 
in a pipeline.
   </p>
   <p>
DUCC's resource manager uses the combination of specified memory requirement and
process class to find space to run each new managed process instance. 
The resource manager does not overcommit RAM memory for machines, and DUCC uses Linux 
cgroups to constrain managed processes from interfering with each other.
Only processes that exceed their requested memory allocation will be subject to paging.
The resource manager employs process preemption to dynamically rebalance compute
resources between collection processing jobs.
   </p>
   <p>
DUCC is primarily intended to be used for research and development activities where
multiple users need to efficiently share cluster resources for a wide variety of
computational activities. All processes run with
the credentials of the submitting user. Process logfiles and DUCC collected performance
data are stored in user filesystem space. This allows/forces each
user to decide how to manage the metadata associated work submitted to DUCC.
   </p>
   <p>
The following sections will describe each of the three type of DUCC managed processes:
collection processing jobs, services and arbitary processes.
   </p>
   </subsection>
  
   <subsection name="DUCC Collection Processing Jobs">
   <p>
A classic UIMA pipeline starts with
a Collection Reader (CR) that defines how to segment the input collection into separate
artifacts for analysis, reads the input data, initializes a new CAS with each artifact and 
returns the CAS to be sent to downstream analytic components.
Because a single CR supplying artifacts to a large number of analysis pipelines 
would be a bottleneck, DUCC implements collection level scale out according to the design in
Figure 1. 
   </p>
   <p>
     <table width="100%"><tr><td align="center" valign="middle">
       <img src="./images/getting-started/jobmodel.png" alt="DUCC Collection Processing Job Model" border="0"/>
       <br>Figure 1 - DUCC Collection Processing Job Model</br></td>
      </tr></table>
   </p>
   <p>
In a DUCC collection processing job the role of collection segmentation is
implemented by the CR run in the Job Driver. Figure 1 shows the CR inspecting
the input collection to determine how to segment the data into work items to be sent
to the analysis pipeline. The CR may also inspect the target output location to see
which work items have already been done. Then the CR outputs small CASes containing
references to input work items and the associated output locations.
   </p>
   <p>
DUCC wraps the user's CR in a Job Driver, which sends the CASes to a queue feeding
one or more instances of a Job Process containing the analysis pipeline.
Input data reading, artifact extraction and CAS initialization are implemented by the Cas Multiplier
(CM) running in the Job Process. Each artifact CAS is then passed thru the analysis engine (AE) and
CAS Consumer (CC) components.
   </p>
   <p>
A DUCC job specification includes the number of pipeline instances to run in each Job Process.
Each instance is run in a separate thread.
During the job DUCC will automatically scale the number of Job Processes running based on the
number of number of work items left to do, the number of threads per Job Process, and the 
amount of resources available to the job.
   </p>
  </subsection>

   <subsection name="DUCC Services">
   <p>
A DUCC service is a process whose life cycle DUCC will manage and/or whose health DUCC will monitor.
The service can be configured to 
be automatically started as soon as resources are available, or it can be started on demand, when DUCC is
asked to start a job or another service that have declared a dependency on it. 
Another configuration parameter specifies the default number of service processes to start.
Users can also start or stop services manually, as well as manually change the number of running instances.
   </p> 
   <p>
There are two type of DUCC services: UIMA-AS and CUSTOM.
Any existing UIMA-AS services would be able to use DUCC for service life cycle management.
Each DUCC service must have a "service pinger" class that DUCC will call periodically to
get the status of the service. A dependent job or service will not be given resources to run unless
the service pingers indicate all services it depends on are available.
DUCC has a default pinger for UIMA-AS services which will be used if none are specified.
CUSTOM services must register a pinger class.
   </p> 
   <p>
Each service instance runs in a cgroup, and any spawned subprocesses will also run in the same container.
When a UIMA-AS service instance is stopped DUCC will initiate a quiesce and then 60 seconds later
do hard kill. CUSTOM services will first receive a SIGTERM and then 60 seconds later SIGKILL.
All spawned processes are terminated when the container is removed.
   </p>
   <p>
Services are tracked on the Services page of DUCC webserver.
   </p>
  </subsection>
  
   <subsection name="DUCC Arbitrary Processes">
   <p>
DUCC can be used to run an arbitrary process on a DUCC worker node. 
Resources are allocated according to the memory size and scheduling class requested. 
The process is run in a cgroup.
The allocated resource is freed when the process terminates.
   </p> 
   <p>
A command line script, viaducc, that can be used to launch processes on a DUCC worker node.
With a symlink named java-viaducc->$DUCC_HOME/bin/viaducc placed in $JAVA_HOME/bin, viaducc can
be used to launch arbitrary processes directly from eclipse onto DUCC worker nodes.
   </p>
   <p>
Arbitrary processes are tracked on the Reservations page of DUCC webserver.
   </p>
  </subsection>
  
   <subsection name="DUCC - What next?">
   <p>
Go to ??? and see the full documentation.
   </p> 
   <p>
Go to the <a href="http://uima.apache.org/downloads.cgi">UIMA Download Page</a> 
and get the "UIMA DUCC" install package.
   </p>
   <p>
The DUCC install package includes sample applications that demonstrate two useful
Collection Processing applications.
   </p>
  </subsection>

  </section>

 </body>

</document>

